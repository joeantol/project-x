{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import csv\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    import keras\n",
    "    \n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "from keras_sequential_ascii import sequential_model_to_ascii_printout\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "    \n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping, History, LambdaCallback, ModelCheckpoint\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Conv2D \n",
    "from keras.layers import MaxPooling2D, ZeroPadding2D, BatchNormalization\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Allow image embeding in notebook\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectX:\n",
    "    def __init__ (self, logfile_name='project-x'):\n",
    "        self.logfile_name = logfile_name\n",
    "        self.mean = self.std = []\n",
    "        \n",
    "\n",
    "    def args(self, args):\n",
    "        \n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--aug', '-a', default=True)\n",
    "        parser.add_argument('--batchsize', '-b', default=32)\n",
    "        parser.add_argument('--epochs', '-e', default=500)\n",
    "        parser.add_argument('--height', '-y')\n",
    "        parser.add_argument('--imagedir', '-i')\n",
    "        parser.add_argument('--learningrate', '-l', default=1e-6)\n",
    "        parser.add_argument('--numimages', '-n')\n",
    "        parser.add_argument('--opt', '-o', default='Adam')\n",
    "        parser.add_argument('--projectname', '-p', default='projectx')\n",
    "        parser.add_argument('--saveprefix', '-s', default='project-x')\n",
    "        parser.add_argument('--uniqueid', '-u', default=0)\n",
    "        parser.add_argument('--width', '-x')\n",
    "\n",
    "        print('Args: ' + ' '.join(args) + '\\n')\n",
    "\n",
    "        args = parser.parse_args(args)\n",
    "\n",
    "        try:\n",
    "            get_ipython().__class__.__name__\n",
    "            self.in_jupyter = True\n",
    "            print('In Jupyter...')\n",
    "\n",
    "        except:\n",
    "            args = parser.parse_args()\n",
    "            self.in_jupyter = False\n",
    "            print('NOT in Jupyter...')\n",
    "\n",
    "        self.data_augmentation = args.aug\n",
    "        self.batch_size        = int(args.batchsize)\n",
    "        self.epochs            = int(args.epochs)\n",
    "        self.image_height      = int(args.height)\n",
    "        self.image_dir         = os.path.join(os.getcwd(), args.imagedir)\n",
    "        self.lr                = float(args.learningrate)\n",
    "        self.opt               = args.opt\n",
    "        self.project_name      = args.projectname\n",
    "        self.unique_id         = int(args.uniqueid)\n",
    "        self.image_width       = int(args.width)\n",
    "        self.save_prefix       = args.saveprefix\n",
    "\n",
    "        if not self.in_jupyter:\n",
    "            log_file = os.path.join(os.getcwd(), 'logs', self.logfile_name + '-' \n",
    "                                    + str(self.unique_id) + '.log')\n",
    "            print(\"Log file: \" + log_file)\n",
    "            sys.stdout = open(log_file, 'w')\n",
    "\n",
    "            \n",
    "    def get_classes_from_dirs(self):\n",
    "\n",
    "        self.training_dir   = os.path.join(self.image_dir, 'training')\n",
    "        self.validation_dir = os.path.join(self.image_dir, 'validation')\n",
    "\n",
    "        self.classes = []\n",
    "\n",
    "        for d in os.listdir(self.training_dir):\n",
    "            if os.path.isdir(os.path.join(self.training_dir, d)):\n",
    "                self.classes.append(d)\n",
    "\n",
    "        print(\"Training dir: \" + self.training_dir)\n",
    "        print(\"Validation dir: \" + self.validation_dir + '\\n')\n",
    "        print(\"Classes: \" + str(self.classes) + '\\n')\n",
    "\n",
    "        self.num_images = 0\n",
    "\n",
    "        for dirs, subdirs, files in os.walk(self.training_dir):\n",
    "            for file in files:\n",
    "                self.num_images += 1\n",
    "\n",
    "        for dirs, subdirs, files in os.walk(self.validation_dir):\n",
    "            for file in files:\n",
    "                self.num_images += 1\n",
    "\n",
    "        print('Number of images: ' + str(self.num_images) + '\\n')\n",
    "\n",
    "        \n",
    "    def create_model_dir(self, program_name):\n",
    "\n",
    "        cwd = os.getcwd()\n",
    "        save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "        model_path = os.path.join(save_dir, self.project_name + '-' + self.opt + '-' + str(self.num_images) + '-' \n",
    "                                  + str(self.image_width) + 'x' + str(self.image_height) + '-' + str(self.unique_id))\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "        #... Make a copy of the main script and library\n",
    "        if not self.in_jupyter:\n",
    "            shutil.copy2(__file__, model_path)\n",
    "            shutil.copy2(program_name, model_path)\n",
    "\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        print('Saving model at: '+ self.model_path)\n",
    "        \n",
    "\n",
    "    def create_generators(self, datagen, image_dir):\n",
    "        \n",
    "        image_width    = self.image_width\n",
    "        image_height   = self.image_height\n",
    "        batch_size     = self.batch_size\n",
    "                \n",
    "        gen = datagen.flow_from_directory(\n",
    "                image_dir,\n",
    "                target_size=(image_width, image_height),\n",
    "                batch_size=batch_size,\n",
    "                class_mode='categorical',\n",
    "                follow_links=True,\n",
    "                color_mode='rgb',\n",
    "                shuffle=True,\n",
    "                seed=42,\n",
    "        )\n",
    "        \n",
    "        return gen\n",
    "\n",
    "    def train_the_model (self, model, train_gen, val_gen, epochs=500):\n",
    "\n",
    "        print(\"Train the model...\")\n",
    "        \n",
    "        batch_size = self.batch_size\n",
    "        model_path = self.model_path\n",
    "              \n",
    "        hist = History()\n",
    "        early_stopping = EarlyStopping(monitor='val_acc', patience=35, verbose=2, mode='auto')\n",
    "        time_callback = TimeHistory()\n",
    "        lambda_callback = LambdaCallback(on_batch_end=lambda batch,logs:print(logs))\n",
    "\n",
    "        checkpoint_file = os.path.join(model_path, 'model.{epoch:02d}-{val_acc:.2f}.hdf5')\n",
    "        model_checkpoint = ModelCheckpoint(checkpoint_file, monitor='val_acc', verbose=1, save_best_only=True, \n",
    "                                           save_weights_only=False, mode='auto', period=1)\n",
    "        \n",
    "        hist = model.fit_generator(\n",
    "                train_gen,\n",
    "                steps_per_epoch=train_gen.samples//batch_size + 1,\n",
    "                epochs=epochs,\n",
    "                validation_data=val_gen,\n",
    "                validation_steps=val_gen.samples//batch_size + 1,\n",
    "                use_multiprocessing=True,\n",
    "                workers=8,\n",
    "                callbacks=[early_stopping, time_callback, model_checkpoint]\n",
    "        )\n",
    "\n",
    "        self.hist = hist\n",
    "        self.time_callback = time_callback\n",
    "        self.label_map = (train_gen.class_indices)\n",
    "\n",
    "        \n",
    "    def save_the_model(self, model, val_gen):\n",
    "        \n",
    "        model_path    = self.model_path\n",
    "        hist          = self.hist\n",
    "        time_callback = self.time_callback\n",
    "        label_map     = self.label_map\n",
    "\n",
    "        #... Save the architecture\n",
    "#         model_json = model.to_json()\n",
    "#         with open(os.path.join(model_path, 'model_arch.json'), \"w\") as json_file:\n",
    "#             json_file.write(model_json)\n",
    "#         print('Saved model architecture at %s ' % model_path)\n",
    "            \n",
    "#         #... Save the weights\n",
    "#         model.save_weights(os.path.join(model_path, 'model_weights.hdf5'))\n",
    "#         print('Saved model weights at %s ' % model_path)\n",
    "            \n",
    "        #... Save model and weights\n",
    "        model.save(os.path.join(model_path, 'model.hdf5'))\n",
    "        print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "        #... Save history\n",
    "        with open(os.path.join(model_path, 'history.pk'), 'wb') as f:\n",
    "            pickle.dump(hist.history, f)\n",
    "        print('Saved history at %s ' % model_path)\n",
    "\n",
    "        #... Save epoch times\n",
    "        with open(os.path.join(model_path, 'times.pk'), 'wb') as f:\n",
    "            pickle.dump(time_callback.times, f)\n",
    "        print('Saved epoch times at %s ' % model_path)\n",
    "\n",
    "        print('Score the trained model...')\n",
    "        pred = model.predict_generator(val_gen, workers=8, use_multiprocessing=True, verbose=1)\n",
    "        with open(os.path.join(model_path, 'pred.pk'), 'wb') as f:\n",
    "            pickle.dump(pred, f)\n",
    "        print('Saved predictions at %s ' % model_path)\n",
    "\n",
    "        eval_scores = model.evaluate_generator(val_gen, workers=8, use_multiprocessing=True)\n",
    "        with open(os.path.join(model_path, 'eval.pk'), 'wb') as f:\n",
    "            pickle.dump(eval_scores, f)\n",
    "        print('Saved eval at %s ' % model_path)\n",
    "\n",
    "        with open(os.path.join(model_path, 'label_map.pk'), 'wb') as f:\n",
    "            pickle.dump(label_map, f)\n",
    "        print('Saved label map at %s ' % model_path)\n",
    "\n",
    "        print('Test loss:', eval_scores[0])\n",
    "        print('Test accuracy:', eval_scores[1])\n",
    "        \n",
    "        \n",
    "    def load_the_model(self, model_dir, model_file='model.hdf5', model_arch='model_arch.json', model_weights='model_weights.hdf5'):\n",
    "        \n",
    "        model_stats = {}\n",
    "        \n",
    "        model_name, opt, num_images, image_size, unique_id = model_dir.split('-')\n",
    "        width, height = image_size.split('x')\n",
    "        image_width = int(width)\n",
    "        image_height = int(height)\n",
    "        \n",
    "        model_stats['image_width']  = image_width\n",
    "        model_stats['image_height'] = image_height\n",
    "        \n",
    "        cwd = os.getcwd()\n",
    "        save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "        model_path = os.path.join(save_dir, model_dir)\n",
    "        \n",
    "        # load json and create model\n",
    "#         with open(os.path.join(model_path, model_arch), 'r') as json_file:\n",
    "#             model_json = json_file.read()\n",
    "#             model = model_from_json(model_json)\n",
    "#             model_stats['model'] = model\n",
    "#             print('Loaded model architecture from %s ' % model_path)\n",
    "\n",
    "#         model.load_weights(os.path.join(model_path, model_weights))\n",
    "#         model_stats['model_weights'] = model_weights\n",
    "#         print('Loaded model weights from %s ' % model_path)\n",
    "\n",
    "        #... Load model and weights\n",
    "        model = load_model(os.path.join(model_path, model_file))\n",
    "        model_stats['model'] = model\n",
    "        print('Loaded trained model from %s ' % model_path)\n",
    "                        \n",
    "        with open(os.path.join(model_path, 'eval.pk'), 'rb') as f:\n",
    "            eval = pickle.load(f)\n",
    "            model_stats['eval'] = eval\n",
    "            print('Loaded evals...')\n",
    "                  \n",
    "        with open(os.path.join(model_path, 'history.pk'), 'rb') as f:\n",
    "            hist= pickle.load(f)\n",
    "            model_stats['hist'] = hist\n",
    "            print('Loaded history...')\n",
    "                  \n",
    "        with open(os.path.join(model_path, 'pred.pk'), 'rb') as f:\n",
    "            pred = pickle.load(f)\n",
    "            model_stats['pred'] = pred\n",
    "            print('Loaded pred...')\n",
    "                  \n",
    "        with open(os.path.join(model_path, 'times.pk'), 'rb') as f:\n",
    "            times = pickle.load(f)\n",
    "            model_stats['times'] = times\n",
    "            print('Loaded times...')\n",
    "                  \n",
    "        with open(os.path.join(model_path, 'label_map.pk'), 'rb') as f:\n",
    "            label_map = pickle.load(f)\n",
    "            model_stats['label_map'] = label_map\n",
    "            print('Loaded label_map...')\n",
    "                  \n",
    "        return model_stats\n",
    "        \n",
    "        \n",
    "    def sample_mean_and_std(self, batch_size=0):\n",
    "\n",
    "        training_dir = os.path.join(self.image_dir, 'training')\n",
    "        image_width  = self.image_width\n",
    "        image_height = self.image_height\n",
    "        batch_size   = self.batch_size if batch_size == 0 else batch_size\n",
    "        \n",
    "        sample_mean = sample_var = sample_std = 0\n",
    "        num_batches = num_samples = 0\n",
    "        \n",
    "        stats_file = os.path.join(training_dir, 'train_stats.pk')\n",
    "        \n",
    "        dg = ImageDataGenerator()\n",
    "\n",
    "        tg = dg.flow_from_directory(\n",
    "                training_dir,\n",
    "                target_size=(image_width, image_height),\n",
    "                batch_size=batch_size,\n",
    "                class_mode='categorical',\n",
    "                follow_links=True,\n",
    "        )\n",
    "\n",
    "        if os.path.isfile(stats_file):\n",
    "            print(\"Reading existing stats file...\")\n",
    "            with open(stats_file, 'rb') as f:\n",
    "                num_samples = pickle.load(f)\n",
    "                sample_mean = pickle.load(f)\n",
    "                sample_std  = pickle.load(f)\n",
    "        else:\n",
    "            print(\"Stats file does not exist...\")\n",
    "            \n",
    "        if num_samples > 0 and num_samples != tg.samples:\n",
    "            print(\"Number of samples differs between current and stored...\") \n",
    "\n",
    "        if num_samples != tg.samples:\n",
    "            \n",
    "            print(\"Recalculating training mean and std...\")\n",
    "            \n",
    "            for img, label in tg:\n",
    "                sample_mean += img.mean(axis=(1,2), keepdims=True).mean(axis=0, keepdims=True)\n",
    "                sample_var  += ((img.std(axis=(1,2), keepdims=True))**2).mean(axis=0, keepdims=True)\n",
    "\n",
    "                num_batches += 1\n",
    "\n",
    "                print(num_batches, img.mean(), sample_mean/num_batches, \n",
    "                      img.std(), (np.sqrt(sample_var / num_batches)))\n",
    "\n",
    "                #... Generators are in an infinite loop\n",
    "                if num_batches*batch_size > tg.samples:\n",
    "                    break\n",
    "\n",
    "            sample_mean /= num_batches\n",
    "            sample_std  = (np.sqrt(sample_var / num_batches) + K.epsilon())\n",
    "\n",
    "            with open(stats_file, 'wb') as f:\n",
    "                pickle.dump(tg.samples, f)\n",
    "                pickle.dump(sample_mean, f)\n",
    "                pickle.dump(sample_std, f)\n",
    "\n",
    "        return sample_mean, sample_std\n",
    "    \n",
    "    \n",
    "    def normalize(self, x):\n",
    "        \n",
    "        if self.mean.size == 0 or self.std.size == 0:\n",
    "            raise ValueError('Training mean and/or std is less than zero...')\n",
    "\n",
    "        x = (x - self.mean)/self.std\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def greyscale(self, x):\n",
    "        x[:] = x.mean(axis=-1,keepdims=1)\n",
    "        \n",
    "        x = self.normalize(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#... TODO: Confustion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('In main...')\n",
    "\n",
    "    X = ProjectX()\n",
    "    \n",
    "    args = ['-x=224', '-y=224', '-i=data/union/gunks/trapps/trainval', '-u=110', '-l=1e-4']\n",
    "    \n",
    "    X.args(args)\n",
    "        \n",
    "#     sample_mean, sample_std = X.sample_mean_and_std(batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python363",
   "language": "python",
   "name": "python363"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
